{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Cross-Entropy Loss\n",
    "\n",
    "Cross-entropy is a measure of the difference between two probability distributions. It's often used in machine learning as a loss function for classification problems.\n",
    "\n",
    "For multi-class classification problems, the cross-entropy loss is calculated as the negative log likelihood of the correct class. For target probability distribution $p^t$ and the predicted probability distribution $p^y$, cross-entropy is defined as:\n",
    "\n",
    "$$-p^t\\cdot log(p^y) = - \\sum_{i=1}^{\\|V\\|}p^t_i\\cdot log(p^y_i)$$\n",
    "\n",
    "Because we want the model to assign a probability of 1 to the correct token, and zero to other tokens, the target probability is set to\n",
    "\n",
    "$$p^t = \\textrm{one-hot}(y^t)$$\n",
    "\n",
    "where $y^t$ is the target token. In other words, $p^t$ is a vector with 1 for the correct token and 0 for all other tokens. \n",
    "\n",
    "Thus, the lower the cross-entropy, the better the model's predictions. A perfect model would have a cross-entropy of 0.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
