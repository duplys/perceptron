{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training an MLP\n",
    "\n",
    "This notebook is based on [Andrej Karpathy's excellent micrograd tutorial](https://github.com/karpathy/micrograd) and Joerg Frochte's book \"Maschinelles Lernen: Grundlagen und Algorithmen in Python\", 3rd edition."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recap: Multi-Layer Perceptron (MLP)\n",
    "Recall that a perceptron can only distinguish between members of class A and members of class B **if** these **classes are linearly separable**.\n",
    "\n",
    "In the 1960, this led to a search for classification approaches using **multi-layer perceptrons**. The researchers found that an MLP with just **2 adjustable layers** of weights can correctly classify **any finite number of classes** if there is a sufficient number of \"hidden\" perceptrons in the layer mediating between the input vector and the output unit.\n",
    "\n",
    "Unfortunately, for fewer hidden perceptrons than cases to be learned, no even moderately efficient algorithm for systematically computing the weights could be found. Eventually, researchers came up with an algorithm known as **backpropagation** (**BP**) based on the concept of derivation known from calculus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<!-- Generated by graphviz version 11.0.0 (20240428.1522)\n",
       " -->\n",
       "<!-- Title: multi&#45;layer&#45;perceptron&#45;simplified Pages: 1 -->\n",
       "<svg width=\"439pt\" height=\"299pt\"\n",
       " viewBox=\"0.00 0.00 438.95 299.47\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 295.47)\">\n",
       "<title>multi&#45;layer&#45;perceptron&#45;simplified</title>\n",
       "<polygon fill=\"white\" stroke=\"none\" points=\"-4,4 -4,-295.47 434.95,-295.47 434.95,4 -4,4\"/>\n",
       "<!-- I1 -->\n",
       "<g id=\"node1\" class=\"node\">\n",
       "<title>I1</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"24.74\" cy=\"-266.74\" rx=\"24.74\" ry=\"24.74\"/>\n",
       "<text text-anchor=\"middle\" x=\"24.74\" y=\"-261.69\" font-family=\"Times,serif\" font-size=\"14.00\">x_1</text>\n",
       "</g>\n",
       "<!-- H1 -->\n",
       "<g id=\"node4\" class=\"node\">\n",
       "<title>H1</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"218.21\" cy=\"-266.74\" rx=\"24.74\" ry=\"24.74\"/>\n",
       "<text text-anchor=\"middle\" x=\"218.21\" y=\"-261.69\" font-family=\"Times,serif\" font-size=\"14.00\">h_1</text>\n",
       "</g>\n",
       "<!-- I1&#45;&gt;H1 -->\n",
       "<g id=\"edge1\" class=\"edge\">\n",
       "<title>I1&#45;&gt;H1</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M49.6,-266.74C82.78,-266.74 143.1,-266.74 181.61,-266.74\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"181.53,-270.24 191.53,-266.74 181.53,-263.24 181.53,-270.24\"/>\n",
       "</g>\n",
       "<!-- H2 -->\n",
       "<g id=\"node5\" class=\"node\">\n",
       "<title>H2</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"218.21\" cy=\"-145.74\" rx=\"24.74\" ry=\"24.74\"/>\n",
       "<text text-anchor=\"middle\" x=\"218.21\" y=\"-140.69\" font-family=\"Times,serif\" font-size=\"14.00\">h_2</text>\n",
       "</g>\n",
       "<!-- I1&#45;&gt;H2 -->\n",
       "<g id=\"edge2\" class=\"edge\">\n",
       "<title>I1&#45;&gt;H2</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M46.23,-253.79C80.13,-232.36 148.04,-189.45 187.31,-164.64\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"189.01,-167.7 195.59,-159.4 185.27,-161.79 189.01,-167.7\"/>\n",
       "</g>\n",
       "<!-- H3 -->\n",
       "<g id=\"node6\" class=\"node\">\n",
       "<title>H3</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"218.21\" cy=\"-24.74\" rx=\"24.74\" ry=\"24.74\"/>\n",
       "<text text-anchor=\"middle\" x=\"218.21\" y=\"-19.69\" font-family=\"Times,serif\" font-size=\"14.00\">h_3</text>\n",
       "</g>\n",
       "<!-- I1&#45;&gt;H3 -->\n",
       "<g id=\"edge3\" class=\"edge\">\n",
       "<title>I1&#45;&gt;H3</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M40.76,-247.75C74.31,-205.35 155.44,-102.81 194.9,-52.93\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"197.59,-55.17 201.05,-45.16 192.1,-50.83 197.59,-55.17\"/>\n",
       "</g>\n",
       "<!-- I2 -->\n",
       "<g id=\"node2\" class=\"node\">\n",
       "<title>I2</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"24.74\" cy=\"-145.74\" rx=\"24.74\" ry=\"24.74\"/>\n",
       "<text text-anchor=\"middle\" x=\"24.74\" y=\"-140.69\" font-family=\"Times,serif\" font-size=\"14.00\">x_2</text>\n",
       "</g>\n",
       "<!-- I2&#45;&gt;H1 -->\n",
       "<g id=\"edge4\" class=\"edge\">\n",
       "<title>I2&#45;&gt;H1</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M46.23,-158.69C80.13,-180.11 148.04,-223.03 187.31,-247.84\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"185.27,-250.69 195.59,-253.07 189.01,-244.77 185.27,-250.69\"/>\n",
       "</g>\n",
       "<!-- I2&#45;&gt;H2 -->\n",
       "<g id=\"edge5\" class=\"edge\">\n",
       "<title>I2&#45;&gt;H2</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M49.6,-145.74C82.78,-145.74 143.1,-145.74 181.61,-145.74\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"181.53,-149.24 191.53,-145.74 181.53,-142.24 181.53,-149.24\"/>\n",
       "</g>\n",
       "<!-- I2&#45;&gt;H3 -->\n",
       "<g id=\"edge6\" class=\"edge\">\n",
       "<title>I2&#45;&gt;H3</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M46.23,-132.79C80.13,-111.36 148.04,-68.45 187.31,-43.64\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"189.01,-46.7 195.59,-38.4 185.27,-40.79 189.01,-46.7\"/>\n",
       "</g>\n",
       "<!-- I3 -->\n",
       "<g id=\"node3\" class=\"node\">\n",
       "<title>I3</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"24.74\" cy=\"-30.74\" rx=\"18\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"24.74\" y=\"-25.69\" font-family=\"Times,serif\" font-size=\"14.00\">1</text>\n",
       "</g>\n",
       "<!-- I3&#45;&gt;H1 -->\n",
       "<g id=\"edge7\" class=\"edge\">\n",
       "<title>I3&#45;&gt;H1</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M36.86,-44.45C67.52,-82.23 153.67,-188.42 194.81,-239.12\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"191.97,-241.18 200.99,-246.74 197.41,-236.77 191.97,-241.18\"/>\n",
       "</g>\n",
       "<!-- I3&#45;&gt;H2 -->\n",
       "<g id=\"edge8\" class=\"edge\">\n",
       "<title>I3&#45;&gt;H2</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M40.76,-39.76C72.13,-58.6 145.13,-102.44 186.74,-127.43\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"184.92,-130.43 195.3,-132.58 188.53,-124.43 184.92,-130.43\"/>\n",
       "</g>\n",
       "<!-- I3&#45;&gt;H3 -->\n",
       "<g id=\"edge9\" class=\"edge\">\n",
       "<title>I3&#45;&gt;H3</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M43.07,-30.19C74.32,-29.22 140.69,-27.14 181.87,-25.84\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"181.87,-29.35 191.76,-25.54 181.65,-22.35 181.87,-29.35\"/>\n",
       "</g>\n",
       "<!-- O -->\n",
       "<g id=\"node7\" class=\"node\">\n",
       "<title>O</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"408.95\" cy=\"-145.74\" rx=\"18\" ry=\"18\"/>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"408.95\" cy=\"-145.74\" rx=\"22\" ry=\"22\"/>\n",
       "<text text-anchor=\"middle\" x=\"408.95\" y=\"-140.69\" font-family=\"Times,serif\" font-size=\"14.00\">o</text>\n",
       "</g>\n",
       "<!-- H1&#45;&gt;O -->\n",
       "<g id=\"edge10\" class=\"edge\">\n",
       "<title>H1&#45;&gt;O</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M239.41,-253.79C273.37,-232.02 341.94,-188.06 380.33,-163.45\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"382.04,-166.5 388.57,-158.16 378.27,-160.61 382.04,-166.5\"/>\n",
       "</g>\n",
       "<!-- H2&#45;&gt;O -->\n",
       "<g id=\"edge11\" class=\"edge\">\n",
       "<title>H2&#45;&gt;O</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M243.12,-145.74C276.59,-145.74 337.55,-145.74 375.22,-145.74\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"375.05,-149.24 385.05,-145.74 375.05,-142.24 375.05,-149.24\"/>\n",
       "</g>\n",
       "<!-- H3&#45;&gt;O -->\n",
       "<g id=\"edge12\" class=\"edge\">\n",
       "<title>H3&#45;&gt;O</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M239.41,-37.69C273.37,-59.46 341.94,-103.42 380.33,-128.03\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"378.27,-130.86 388.57,-133.32 382.04,-124.97 378.27,-130.86\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<graphviz.graphs.Digraph at 0x1044c8940>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "from graphviz import Digraph\n",
    "\n",
    "# Create a new directed graph\n",
    "dot = Digraph('multi-layer-perceptron-simplified', graph_attr={'rankdir': 'LR', 'ranksep': '2', 'nodesep': '1'}, comment='Multi-layer perceptron (simplified)')\n",
    "\n",
    "# Add nodes for the input layer\n",
    "for i in range(1, 3):\n",
    "    dot.node(f'I{i}', f'x_{i}', shape='circle')\n",
    "\n",
    "dot.node(f'I3', '1', shape='circle')\n",
    "\n",
    "# Add nodes for the hidden layer\n",
    "for i in range(1, 4):\n",
    "    dot.node(f'H{i}', f'h_{i}', shape='circle')\n",
    "\n",
    "# Add node for the output layer\n",
    "dot.node('O', 'o', shape='doublecircle')\n",
    "\n",
    "# Connect the nodes from the input layer to the hidden layer\n",
    "for i in range(1, 3):\n",
    "    for j in range(1, 4):\n",
    "        dot.edge(f'I{i}', f'H{j}')\n",
    "\n",
    "dot.edge(f'I3', f'H1')\n",
    "dot.edge(f'I3', f'H2')\n",
    "dot.edge(f'I3', f'H3')\n",
    "\n",
    "# Connect the nodes from the hidden layer to the output layer\n",
    "for i in range(1, 4):\n",
    "    dot.edge(f'H{i}', 'O')\n",
    "\n",
    "# Show the graph\n",
    "dot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recap: Backpropagation\n",
    "\n",
    "Recall that training a Multi-Layer-Perceptron (MLP) requires an algorithm known as **backpropagation** (**BP**). BP is essentially the **recursive application of the chain rule backwards through the propagation graph**. The chain rule says that if a variable $z$ depends on variable $y$, which, in turn, depends on the variable $x$, then $z$'s dependence on $x$ is:\n",
    "\n",
    "$$\\frac{dz}{dx} = \\frac{dz}{dy} \\cdot \\frac{dy}{dx}$$\n",
    "\n",
    "So, BP first computes the function value and, using this value, computes backwards the gradients (i.e., the partial derivatives) for all intermediate nodes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the MLP\n",
    "\n",
    "To train an MLP, we must find weights $W$ that give the best result. But how can we measure the performance of an MLP? Well, similar to a simple perceptron, for a given input $x$, we want the MLP to compute the output $y$ which is equal to the desired output $y_d$. Thus, given a **training set** $D = \\{X, Y\\}$, we want to **minimize** the [residual sum of squares](https://en.wikipedia.org/wiki/Residual_sum_of_squares)\n",
    "\n",
    "$$L(W)=\\sum_{y_D\\in Y} \\frac{1}{2} (y_D - y)^2$$\n",
    "\n",
    "Note that $L(W)$ depends only on the weights $W$ that the MLP learns. Moreover, it makes sense to minimize $L(W)$ because, ultimately, if we could come up with weights $W$ for which the MLP always produces the correct output $y$, then $L(W)$ would be $0$. Hence, $L(W)$ is a single number to measure MLP's performance and it's called a **loss** or **loss function**.\n",
    "\n",
    "If we want to use gradients (i.e., gradient descent) to find the minimum of $L(W)$, we must update the weights as:\n",
    "\n",
    "$$W_{new} = W_{old} - \\sigma \\nabla L(W_{old})$$\n",
    "\n",
    "where $\\nabla$ is the gradient of $L$ (i.e., a vector of partial derivates of all inputs of $L$):\n",
    "\n",
    "$$\\nabla = \\left(\\frac{\\partial L}{\\partial x_1}, \\frac{\\partial L}{\\partial x_2}, \\ldots, \\frac{\\partial L}{\\partial x_n}\\right)$$\n",
    "\n",
    "Because the $\\nabla$-operator is linear (and because we can apply the chain rule), it holds that:\n",
    "\n",
    "$$\\nabla L(W)=\\sum_{y_D\\in Y} \\frac{1}{2} \\nabla (y_D - y)^2 = \\sum_{y_D\\in Y} \\nabla (y_D - y)\\cdot (-\\nabla y) = - \\sum_{y_D\\in Y} (y_D - y) \\nabla y$$\n",
    "\n",
    "Using BP, we can compute these partial deliverables and, as a result, update the weights $W_{new}$ in order to learn the weights that minimize the loss function $L$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training MLP, Spelled Out in Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import random\n",
    "\n",
    "class Value:\n",
    "    \"\"\"Class that represents a value that can be differentiated in respect to other values.\n",
    "\n",
    "    Attributes:\n",
    "        data: The value itself.\n",
    "        _prev: A set of values that this value is dependent on.\n",
    "        _op: The operation that produced this value.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, data, _children=(), _op='', label='') -> None:\n",
    "        self.data = data\n",
    "        self.grad = 0.0 # derivative of the loss function in respect to this value\n",
    "        self._backward = lambda: None # defines how to propagate the output gradient to the input gradient\n",
    "        self._prev = set(_children)\n",
    "        self._op = _op\n",
    "        self.label = label\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        return f\"Value(data={self.data}, label={self.label})\"\n",
    "    \n",
    "    def __add__(self, other):\n",
    "        def _backward():\n",
    "            # For addition, the local derivative is always 1. So, given the expression a = b + c, it follows from the chain rule \n",
    "            # dz/dx = dz/dy*dy/dx that the gradients of b and c are (1.0 * the gradient of a)\n",
    "            if isinstance(other, Value):\n",
    "                self.grad += 1.0 * out.grad\n",
    "                other.grad += 1.0 * out.grad\n",
    "            else:\n",
    "                #print(f\"__add__ backward for {self} and {other}\")\n",
    "                #print(f\"uid(self)={id(self)}, uid(other)={id(other)}\")\n",
    "                self.grad += 1.0 * out.grad\n",
    "\n",
    "        if isinstance(other, Value):\n",
    "            out = Value(self.data + other.data, (self, other), '+')\n",
    "            out._backward = _backward\n",
    "            return out\n",
    "        else:\n",
    "            out = Value(self.data + other, (self,), '+')\n",
    "            out._backward = _backward\n",
    "            return out\n",
    "        \n",
    "    def __radd__(self, other):\n",
    "        return self + other\n",
    "        \n",
    "    def __neg__(self): #-self\n",
    "        return self * -1\n",
    "        \n",
    "    def __sub__(self, other): #self - other, uses __neg__ and __add__\n",
    "        return self + (-other)\n",
    "        \n",
    "    def __mul__(self, other):\n",
    "        def _backward():\n",
    "            # For multiplication, the local derivative is always the other value (because for z=x*y, dz/dx = y). \n",
    "            # So, given the expression a = b * c, it follows from the chain rule that the gradient of b\n",
    "            # is (c * the gradient of a), and the gradient of c is (b * the gradient of a).\n",
    "            if isinstance(other, Value):\n",
    "               self.grad += other.data * out.grad\n",
    "               other.grad += self.data * out.grad\n",
    "            else:\n",
    "                #print(f\"__mul__ backward for {self} and {other}\")\n",
    "                #print(f\"uid(self)={id(self)}, uid(other)={id(other)}\")\n",
    "                self.grad += other * out.grad\n",
    "\n",
    "        if isinstance(other, Value):\n",
    "            out = Value(self.data * other.data, (self, other), '*')\n",
    "            out._backward = _backward\n",
    "            return out\n",
    "        else:\n",
    "            out = Value(self.data * other, (self,), '*')\n",
    "            out._backward = _backward\n",
    "            return out\n",
    "        \n",
    "    def __pow__(self, other):\n",
    "        assert isinstance(other, (int, float)), \"only supporting int/float powers for now\"\n",
    "        out = Value(self.data ** other, (self,), f'**{other}')\n",
    "\n",
    "        def _backward():\n",
    "            #print(f\"__pow__ backward for {self} and {other}\")\n",
    "            #print(f\"uid(self)={id(self)}, uid(other)={id(other)}\")\n",
    "            self.grad += (other * self.data ** (other - 1)) * out.grad\n",
    "        out._backward = _backward\n",
    "\n",
    "        return out\n",
    "\n",
    "    def __rmul__(self, other):\n",
    "        return self * other\n",
    "    \n",
    "    \n",
    "    def __truediv__(self, other):\n",
    "        return self * (other ** -1)\n",
    "        \n",
    "    def tanh(self):\n",
    "        x = self.data\n",
    "        t = (math.exp(2*x) - 1)/(math.exp(2*x) + 1)\n",
    "        out = Value(t, (self,), 'tanh')\n",
    "\n",
    "        def _backward():\n",
    "            #print(f\"__tanh__ backward for {self}\")\n",
    "            #print(f\"uid(self)={id(self)}\")\n",
    "            self.grad += (1 - t**2) * out.grad\n",
    "        out._backward = _backward\n",
    "\n",
    "        return out\n",
    "    \n",
    "    def exp(self):\n",
    "        x = self.data\n",
    "        out = Value(math.exp(x), (self,), 'exp')\n",
    "\n",
    "        def _backward():\n",
    "            #print(f\"__exp__ backward for {self}\")\n",
    "            #print(f\"uid(self)={id(self)}\")\n",
    "            self.grad += out.data * out.grad # because d/dx e^x = e^x\n",
    "        out._backward = _backward\n",
    "\n",
    "        return out\n",
    "    \n",
    "    def backward(self):\n",
    "        topo = []\n",
    "        visited = set()\n",
    "\n",
    "        def build_topo(v):\n",
    "            if v not in visited:\n",
    "                visited.add(v)\n",
    "                for child in v._prev:\n",
    "                    build_topo(child)\n",
    "                topo.append(v)\n",
    "\n",
    "        build_topo(self)\n",
    "        \n",
    "        self.grad = 1.0\n",
    "\n",
    "        for node in reversed(topo):\n",
    "            #print(f\"In Value.backward(): processing node {node} of type {type(node)}\")\n",
    "            node._backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the `Value` class, we can now define the class `Neuron` to represent a single artificial neuron:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Neuron:\n",
    "    \"\"\"Implementation of a neuron in a neural network.\"\"\"\n",
    "\n",
    "    def __init__(self, nin):\n",
    "        self.w = [Value(random.uniform(-1, 1), label='w') for _ in range(nin)]\n",
    "        self.b = Value(random.uniform(-1, 1), label='b')\n",
    "\n",
    "    def __call__(self, x):\n",
    "        # compute raw activation w * x + b\n",
    "        act = sum((wi * xi for wi, xi in zip(self.w, x)), self.b)\n",
    "        out = act.tanh()\n",
    "        return out\n",
    "\n",
    "    def parameters(self):\n",
    "        return self.w + [self.b]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the `Neuron` class in place, we can define the class `Layer` to represent a layer of neurons:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer:\n",
    "    \"\"\"Implementation of a layer in a neural network.\"\"\"\n",
    "\n",
    "    def __init__(self, nin, nout):\n",
    "        \"\"\"Initialize a layer of nout neurons, each taking nin inputs.\"\"\"\n",
    "        self.neurons = [Neuron(nin) for _ in range(nout)]\n",
    "\n",
    "    def __call__(self, x):\n",
    "        outs = [n(x) for n in self.neurons]\n",
    "        return outs[0] if len(outs) == 1 else outs\n",
    "    \n",
    "    def parameters(self):\n",
    "        params = []\n",
    "        for n in self.neurons:\n",
    "            params.extend(n.parameters())\n",
    "\n",
    "        return params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can define an MLP:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP:\n",
    "    \"\"\"Implementation of a multilayer perceptron.\"\"\"\n",
    "\n",
    "    def __init__(self, nin, nouts):\n",
    "        \"\"\"Initialize a MLP with nouts layers, each taking nin inputs.\n",
    "\n",
    "        Args:\n",
    "            nin: Number of inputs to the MLP.\n",
    "            nouts: List of number of neurons in each layer (including the output layer).\n",
    "        \"\"\"\n",
    "        sz = [nin] + nouts\n",
    "        self.layers = [Layer(sz[i], sz[i+1]) for i in range(len(nouts))]\n",
    "\n",
    "    def __call__(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x\n",
    "    \n",
    "    def parameters(self):\n",
    "        return [p for layer in self.layers for p in layer.parameters()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With that we can create an MLP that has 3 inputs, to hidden layers with 4 neurons each, and 1 output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Value(data=0.23637993216219444, label=w),\n",
       " Value(data=-0.9317756352715012, label=w),\n",
       " Value(data=0.6731201416549839, label=w),\n",
       " Value(data=0.9487144513657142, label=b),\n",
       " Value(data=0.991129442483893, label=w),\n",
       " Value(data=0.7911943990965784, label=w),\n",
       " Value(data=0.7965899246198038, label=w),\n",
       " Value(data=-0.3213548836875648, label=b),\n",
       " Value(data=0.2439336329796098, label=w),\n",
       " Value(data=0.08384006885960393, label=w),\n",
       " Value(data=-0.3343776766378621, label=w),\n",
       " Value(data=-0.4730151355604306, label=b),\n",
       " Value(data=-0.37469466322200784, label=w),\n",
       " Value(data=-0.009103997388743235, label=w),\n",
       " Value(data=0.9799288253819181, label=w),\n",
       " Value(data=-0.33009951852483743, label=b),\n",
       " Value(data=0.13846401203000425, label=w),\n",
       " Value(data=-0.00916477252167458, label=w),\n",
       " Value(data=0.43597340400810647, label=w),\n",
       " Value(data=0.7436936358929318, label=w),\n",
       " Value(data=0.48644207943014295, label=b),\n",
       " Value(data=-0.4107845773288499, label=w),\n",
       " Value(data=-0.3292392123888963, label=w),\n",
       " Value(data=0.5395014411311128, label=w),\n",
       " Value(data=0.7174197582632125, label=w),\n",
       " Value(data=-0.09979544772300608, label=b),\n",
       " Value(data=0.9913224775835112, label=w),\n",
       " Value(data=-0.8852056550992426, label=w),\n",
       " Value(data=-0.2504517167030944, label=w),\n",
       " Value(data=0.7479315080879001, label=w),\n",
       " Value(data=0.9515304148848829, label=b),\n",
       " Value(data=0.3214018145689621, label=w),\n",
       " Value(data=-0.7728830746135502, label=w),\n",
       " Value(data=-0.18237616789590438, label=w),\n",
       " Value(data=0.6362951815739042, label=w),\n",
       " Value(data=0.5094940120691511, label=b),\n",
       " Value(data=-0.5644292854152426, label=w),\n",
       " Value(data=0.9459793938541745, label=w),\n",
       " Value(data=0.9837219331821312, label=w),\n",
       " Value(data=-0.7972912748290615, label=w),\n",
       " Value(data=-0.8967583515705111, label=b)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = [2.0, 3.0, -1.0]\n",
    "m = MLP(3, [4, 4, 1])\n",
    "m(x)\n",
    "m.parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In **gradient descent** we are doing the following steps: \n",
    "\n",
    "1. Forward pass\n",
    "2. Backward pass\n",
    "3. Update the weights\n",
    "\n",
    "And we repeat this loop several times until we are happy with the value of the loss function. Let's see gradient descent in action:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*] Initial predictions: [Value(data=-0.9630024218277902, label=), Value(data=-0.5732968153283398, label=), Value(data=-0.953809158309051, label=), Value(data=-0.9345343054069937, label=)]\n",
      "0 7.780010688563344\n",
      "1 7.490748148068295\n",
      "2 6.789535189050534\n",
      "3 4.016056509649505\n",
      "4 0.7830160638950555\n",
      "5 0.45007366549716454\n",
      "6 0.3127403591282777\n",
      "7 0.23653223756345418\n",
      "8 0.18866834694421786\n",
      "9 0.15607158718747194\n",
      "10 0.1325696890458432\n",
      "11 0.11489248847464466\n",
      "12 0.10115485076514655\n",
      "13 0.09019778478422573\n",
      "14 0.08127160749563558\n",
      "15 0.07387103736677544\n",
      "16 0.06764374500233877\n",
      "17 0.06233693326705325\n",
      "18 0.05776474685558685\n",
      "19 0.053787642983550224\n",
      "20 0.05029891296405319\n",
      "21 0.0472156300426895\n",
      "22 0.044472421251929656\n",
      "23 0.04201708968988261\n",
      "24 0.03980747827506222\n",
      "25 0.03780918419173464\n",
      "26 0.035993867412177125\n",
      "27 0.034337981263556194\n",
      "28 0.032821807528759475\n",
      "29 0.03142871443297446\n",
      "30 0.030144579895212575\n",
      "31 0.028957338795511517\n",
      "32 0.027856624337538043\n",
      "33 0.02683348153849545\n",
      "34 0.025880136534253237\n",
      "35 0.024989809459983327\n",
      "36 0.024156561632285814\n",
      "37 0.023375169941599896\n",
      "38 0.022641022986305513\n",
      "39 0.02195003469737522\n",
      "40 0.021298572123897547\n",
      "41 0.020683394752986008\n",
      "42 0.020101603278376463\n",
      "43 0.0195505961509616\n",
      "44 0.019028032571317077\n",
      "45 0.018531800840876626\n",
      "46 0.018059991191150527\n",
      "47 0.017610872371506714\n",
      "48 0.017182871404808675\n",
      "49 0.016774556023675058\n",
      "50 0.01638461938368891\n",
      "51 0.016011866717698754\n",
      "52 0.01565520365064539\n",
      "53 0.01531362593962635\n",
      "54 0.014986210441150786\n",
      "55 0.014672107138289488\n",
      "56 0.014370532085918358\n",
      "57 0.014080761153470747\n",
      "58 0.013802124462328758\n",
      "59 0.013534001429834897\n",
      "60 0.013275816344388722\n",
      "61 0.01302703440662572\n",
      "62 0.012787158180587555\n",
      "63 0.012555724406355787\n",
      "64 0.012332301132059732\n",
      "65 0.012116485128662325\n",
      "66 0.01190789955563145\n",
      "67 0.011706191849638233\n",
      "68 0.011511031811895257\n",
      "69 0.01132210987273901\n",
      "70 0.011139135514649881\n",
      "71 0.010961835837141363\n",
      "72 0.010789954248897459\n",
      "73 0.010623249274229498\n",
      "74 0.010461493462399333\n",
      "75 0.010304472389645181\n",
      "76 0.010151983744876192\n",
      "77 0.010003836490989928\n",
      "78 0.009859850094638276\n",
      "79 0.009719853818032524\n",
      "80 0.009583686067052916\n",
      "81 0.00945119379052519\n",
      "82 0.00932223192605338\n",
      "83 0.009196662888266302\n",
      "84 0.00907435609574988\n",
      "85 0.008955187533306375\n",
      "86 0.008839039346509793\n",
      "87 0.008725799465820302\n",
      "88 0.008615361257780393\n",
      "89 0.008507623201051188\n",
      "90 0.008402488585254908\n",
      "91 0.008299865230777753\n",
      "92 0.008199665227856047\n",
      "93 0.008101804693419323\n",
      "94 0.008006203544300426\n",
      "95 0.007912785285545423\n",
      "96 0.007821476812666714\n",
      "97 0.007732208226782555\n",
      "98 0.0076449126616766165\n",
      "99 0.007559526121893147\n",
      "[*] Final predictions: [Value(data=0.9610223820973547, label=), Value(data=-0.9624870068269291, label=), Value(data=-0.9562351761732065, label=), Value(data=0.9486760467978226, label=)]\n"
     ]
    }
   ],
   "source": [
    "m = MLP(3, [4, 4, 1])\n",
    "\n",
    "# Inputs to the MLP\n",
    "xs = [\n",
    "    [2.0, 3.0,  -1.0],\n",
    "    [3.0, -1.0, 0.5],\n",
    "    [0.5, 1.0, 1.0],\n",
    "    [1.0, 1.0, -1.0],\n",
    "]\n",
    "\n",
    "# Desired outputs (target outputs)\n",
    "ys = [1.0, -1.0, -1.0, 1.0]\n",
    "\n",
    "# Initial predictions of the MLP\n",
    "ypred = [m(x) for x in xs]\n",
    "print(f\"[*] Initial predictions: {ypred}\")\n",
    "\n",
    "learning_rate = 0.05\n",
    "\n",
    "for k in range(100):\n",
    "    # forward pass\n",
    "    ypred = [m(x) for x in xs]\n",
    "    loss = sum((yout - ygt)**2 for yout, ygt in zip(ypred, ys))\n",
    "\n",
    "    # backward pass\n",
    "    # don't forget to reset the gradients!) if the problem is very simple, without resetting the gradients, the gradients will accumulate, but the descent will be faster...\n",
    "    for p in m.parameters():\n",
    "        p.grad = 0.0\n",
    "    loss.backward()\n",
    "\n",
    "    # update parameters\n",
    "    for p in m.parameters():\n",
    "        p.data -= learning_rate * p.grad # update the parameters (gradients)\n",
    "\n",
    "    print(k, loss.data)\n",
    "\n",
    "# What predictions does the model make now, after k rounds of gradient descent?\n",
    "# Remember that the target outputs are 1.0, -1.0, -1.0, 1.0\n",
    "ypred = [m(x) for x in xs]\n",
    "print(f\"[*] Final predictions: {ypred}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
